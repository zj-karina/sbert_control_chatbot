{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47fd4d17-7db4-4a7c-907e-135fe3ad9887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(55083, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=55083, bias=True)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForPreTraining, AutoTokenizer, BertTokenizer\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(\"zjkarina/LaBSE-instructDialogs\")\n",
    "print(tokenizer.vocab_size)\n",
    "model = AutoModelForPreTraining.from_pretrained(\"zjkarina/LaBSE-instructDialogs\")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc408d5-c49a-467a-8bb1-c8ba36dd46eb",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5a7ad-f75f-401e-9a31-691580660adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"MBZUAI/LaMini-instruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff948a51-9c11-4dbe-bc53-a19a69ad612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset[\"train\"][:4000])\n",
    "df.drop(\"instruction_source\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4e8c3e-c88e-4325-84c5-4be78c708140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:25<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Инициализируем список для хранения всех векторов предложений\n",
    "from tqdm.auto import tqdm\n",
    "all_sentence_embeddings1 = []\n",
    "batch_size = 500\n",
    "# Получаем количество сэмплов\n",
    "num_samples = len(df['response'].tolist())\n",
    "  \n",
    "# Проходим по всем сэмплам по batch_size\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    # Выбираем текущий батч\n",
    "    sentence_batch = df['response'].tolist()[i:i+batch_size]\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input1 = tokenizer(sentence_batch, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output1 = model.bert(**encoded_input1.to('cuda'))\n",
    "\n",
    "    # Perform pooling. In this case, mean pooling\n",
    "    sentence_embeddings1 = torch.nn.functional.normalize(model_output1.pooler_output)\n",
    "\n",
    "    # Добавляем вектора предложений текущего батча в список\n",
    "    all_sentence_embeddings1.append(sentence_embeddings1)\n",
    "\n",
    "# Конкатенируем все вектора предложений\n",
    "sentence_embeddings1 = torch.cat(all_sentence_embeddings1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce57a6-ffd8-4ddf-a2b6-f223685235d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_embeddings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3947fba-ecfc-4d47-ace9-81a1bd385c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [01:14<00:00, 53.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#pas@5\n",
    "cum_sum_1 = 0\n",
    "cum_sum_3 = 0\n",
    "cum_sum_5 = 0\n",
    "cum_sum_10 = 0\n",
    "cum_sum_15 = 0\n",
    "for index in tqdm(df.index):\n",
    "    question = df['instruction'][index]\n",
    "    encoded_question = tokenizer(question, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_out = model.bert(**encoded_question.to('cuda'))\n",
    "    question_embedding = torch.nn.functional.normalize(model_out.pooler_output)\n",
    "    cos_similarities = cosine_similarity(question_embedding.cpu().numpy(), sentence_embeddings1.cpu().numpy())[0]\n",
    "    df[\"rank\"]= cos_similarities\n",
    "    rank_s = df[\"rank\"].sort_values(ascending=False)\n",
    "    if index in rank_s[:1].index:\n",
    "        cum_sum_1 += 1\n",
    "    if index in rank_s[:3].index:\n",
    "        cum_sum_3 += 1\n",
    "    if index in rank_s[:5].index:\n",
    "        cum_sum_5 += 1\n",
    "    if index in rank_s[:10].index:\n",
    "        cum_sum_10 += 1\n",
    "    if index in rank_s[:15].index:\n",
    "        cum_sum_15 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b520919-7ffe-4bca-9c05-fda4b5863c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [01:22<00:00, 48.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reciprocal rank: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RR = 0\n",
    "for index in tqdm(df.index):\n",
    "    try:\n",
    "        question = df['instruction'][index]\n",
    "        encoded_question = tokenizer(question, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_out = model.bert(**encoded_question.to('cuda'))\n",
    "        question_embedding = torch.nn.functional.normalize(model_out.pooler_output)\n",
    "        cos_similarities = cosine_similarity(question_embedding.cpu().numpy(), sentence_embeddings1.cpu().numpy())[0]\n",
    "        df[\"rank\"]= cos_similarities\n",
    "        rank_s = df[\"rank\"].sort_values(ascending=False)\n",
    "        RR += 1/(list(rank_s.index).index(index)+1)\n",
    "    except:\n",
    "        print(index)\n",
    "print(f\"Mean reciprocal rank: {round((1/len(df))*RR,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2daece22-db79-41a9-9c4d-e98271ad6c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат работы p@1: 52 %\n",
      "Результат работы p@3: 67 %\n",
      "Результат работы p@5: 72 %\n",
      "Результат работы p@10: 79 %\n",
      "Результат работы p@15: 82 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Результат работы p@1: {int(100*cum_sum_1/len(df))} %\")\n",
    "print(f\"Результат работы p@3: {int(100*cum_sum_3/len(df))} %\")\n",
    "print(f\"Результат работы p@5: {int(100*cum_sum_5/len(df))} %\")\n",
    "print(f\"Результат работы p@10: {int(100*cum_sum_10/len(df))} %\")\n",
    "print(f\"Результат работы p@15: {int(100*cum_sum_15/len(df))} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b386c3a2-d0eb-4706-bbd3-fea45c3c17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "responce = 'Storms are most common in the North Indian basin during the winter months, from December to March.'\n",
    "encoded_responce = tokenizer(responce, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_out_res = model.bert(**encoded_responce.to('cuda'))\n",
    "sentence_embeddings1 = torch.nn.functional.normalize(model_out_res.pooler_output)\n",
    "question = '''\n",
    "the North Indian basin, storms are most common from April to December, with peaks in May and November.\n",
    "It's important to note that this information pertains specifically to the North Indian basin and may not apply to other regions. \n",
    "Additionally, thunderstorm seasons in the United States, Canada, and the Southern Hemisphere may vary. \n",
    "If you have any further questions or concerns, feel free to ask.\n",
    "'''\n",
    "encoded_question = tokenizer(question, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_out = model.bert(**encoded_question.to('cuda'))\n",
    "question_embedding = torch.nn.functional.normalize(model_out.pooler_output)\n",
    "cos_similarities = cosine_similarity(question_embedding.cpu().numpy(), sentence_embeddings1.cpu().numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "916d4e41-72f5-4d98-878d-354dac09c668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80457914], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
